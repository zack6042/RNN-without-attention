# RNN without Attention

## Overview
This project focuses on using a Recurrent Neural Network (RNN) without the attention mechanism for performing text translation from English to Arabic. The use of RNNs showcases traditional sequence-to-sequence learning applied to language translation tasks.

## Project Features
- **Text Translation:** Translates text from English to Arabic using a sequence-to-sequence model.
- **RNN Implementation:** Utilizes a classic RNN architecture, demonstrating foundational techniques in NLP without the complexity of attention mechanisms.

## Access the Notebook
For a hands-on demonstration and to see the model in action, access the notebook through the following Google Colab link:
- [RNN without Attention Notebook](https://colab.research.google.com/drive/1XZdo4azhrUP5n4IdaE8LI9BkY6iX01DV?usp=sharing#scrollTo=mhkNMJ37e2Py)

## Technologies Used
- **Python:** The core programming language used for implementing the model.
- **TensorFlow:** A comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML, and developers easily build and deploy ML-powered applications.
- **Keras:** A high-level neural networks API, capable of running on top of TensorFlow, used for fast prototyping and state-of-the-art research.

## Getting Started
To get started with this project:
1. Visit the notebook link provided above.
2. Save a copy to your Google Drive by clicking `File` > `Save a copy in Drive`.
3. Follow the instructions and code cells in the notebook to run the model.

## Prerequisites
Ensure you have:
- A Google account for accessing Google Colab.
- Basic knowledge of Python programming and neural networks.

## Usage
The notebook is self-explanatory with detailed comments explaining each step. Hereâ€™s how you can make the most of it:
1. Execute each cell sequentially to understand the flow from data preprocessing to model training.
2. Modify hyperparameters or tweak the model architecture to experiment with improvements or different behaviors.
3. Analyze the results and understand the limitations of using RNN without attention in NLP tasks.

## Contributing
We welcome contributions to improve the project. Suggestions could include:
- Enhancements in the RNN architecture.
- Adding comparison with attention-based models.
- Improving the model's efficiency and accuracy.

To contribute, please fork the repository, make your proposed changes, and submit a pull request.

## License
This project is open-sourced under the MIT License. See the LICENSE file for more information.

## Contact
For any queries or further discussions, please open an issue in this repository or reach out via email.

